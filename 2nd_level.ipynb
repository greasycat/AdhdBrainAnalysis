{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d985076",
   "metadata": {},
   "source": [
    "# Group Level Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "19535e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nipype.pipeline as pe\n",
    "from nipype.interfaces import DataSink\n",
    "import nipype.interfaces.fsl as fsl\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b33d2f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0  control  adhd\n",
      "1  sub-11106        1     0\n",
      "2  sub-11044        1     0\n",
      "3  sub-10524        1     0\n",
      "4  sub-10708        1     0\n",
      "5  sub-10429        1     0\n",
      "            0  control  adhd\n",
      "30  sub-70079        0     1\n",
      "31  sub-70080        0     1\n",
      "32  sub-70081        0     1\n",
      "33  sub-70083        0     1\n",
      "34  sub-70086        0     1\n"
     ]
    }
   ],
   "source": [
    "# Suggested by paper\n",
    "problematic_subjects = [\"sub-10998\", \"sub-10159\", \"sub-70055\", \"sub-70022\", \"sub-70048\", \"sub-10680\"]\n",
    "\n",
    "control_subjects = pd.read_csv(\"control_subjects_ids.txt\", header=None)\n",
    "adhd_subjects = pd.read_csv(\"adhd_subjects_ids.txt\", header=None)\n",
    "\n",
    "# remove problematic subjects from adhd and control subjects\n",
    "control_subjects = control_subjects[~control_subjects[0].isin(problematic_subjects)]\n",
    "adhd_subjects = adhd_subjects[~adhd_subjects[0].isin(problematic_subjects)]\n",
    "\n",
    "# add control column and adhd column to both adhd and control subjects\n",
    "control_subjects['control'] = 1\n",
    "control_subjects['adhd'] = 0\n",
    "\n",
    "adhd_subjects['control'] = 0\n",
    "adhd_subjects['adhd'] = 1\n",
    "\n",
    "subjects = pd.concat([control_subjects, adhd_subjects])\n",
    "\n",
    "print(subjects.head())\n",
    "print(subjects.tail())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3ba411f",
   "metadata": {},
   "outputs": [],
   "source": [
    "contrasts = [['group_mean', 'T', ['control', 'adhd'], [1, -1]]] # FSL compatible\n",
    "regressor = {\"control\": subjects['control'].to_list(), \"adhd\": subjects['adhd'].to_list()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40b8769",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "# get mask files from all subjects\n",
    "mask_files = []\n",
    "masks = np.empty((65,77,49, len(subjects)))\n",
    "current_mask = None\n",
    "for i, subject_id in enumerate(subjects[0]):\n",
    "    mask_file = Path(f\"derivatives/{subject_id}/func/{subject_id}_task-scap_bold_space-MNI152NLin2009cAsym_brainmask.nii.gz\")\n",
    "    if mask_file.exists():\n",
    "        mask = nib.load(mask_file.absolute()) # type: ignore\n",
    "        masks[:,:,:,i] = mask.get_fdata() # type: ignore\n",
    "        current_mask = mask\n",
    "\n",
    "mask = masks.mean(axis=3)\n",
    "mask = np.where(mask > 0.8, 1, 0)\n",
    "mask_image = nib.Nifti1Image(mask, current_mask.affine, current_mask.header) # type: ignore\n",
    "\n",
    "Path(\"working_group\").mkdir(exist_ok=True)\n",
    "mask_file = Path(\"working_group/group_mask.nii.gz\").absolute()\n",
    "mask_image.to_filename(mask_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2833473c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 64 feat dirs\n"
     ]
    }
   ],
   "source": [
    "# get feat dirs\n",
    "feat_dirs = []\n",
    "for subject_id in subjects[0]:\n",
    "    # feat_dir = Path(f\"results/{subject_id}/scap.feat\")\n",
    "    feat_dir = Path(f\"derivatives/task/{subject_id}/scap.feat\")\n",
    "    if feat_dir.exists():\n",
    "        feat_dirs.append(feat_dir)\n",
    "\n",
    "print(\"Loaded\", len(feat_dirs), \"feat dirs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d1061a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 19 Linear Up Load\n",
    "# 21 Linear Down Delay\n",
    "\n",
    "SELECT_CONTRAST = list(range(1, 41))\n",
    "\n",
    "def run_group_level_analysis(contrast_id):\n",
    "    # Get cope files\n",
    "    cope_files = [x.absolute() / \"stats\" / f\"cope{contrast_id}.nii.gz\" for x in feat_dirs if (x / \"stats\" / f\"cope{contrast_id}.nii.gz\").exists()]\n",
    "    print(\"Loaded\", len(cope_files), \"cope files\")\n",
    "\n",
    "    varcopes = [x.absolute() / \"stats\" / f\"varcope{contrast_id}.nii.gz\" for x in feat_dirs if (x / \"stats\" / f\"varcope{contrast_id}.nii.gz\").exists()]\n",
    "    print(\"Loaded\", len(varcopes), \"varcope files\")\n",
    "\n",
    "    print(cope_files[0])\n",
    "\n",
    "    workflow = pe.Workflow(name=\"group_level_analysis\")\n",
    "\n",
    "    merge_cope = pe.Node(fsl.Merge(dimension=\"t\"), name=\"merge_cope\")\n",
    "    merge_cope.inputs.in_files = cope_files\n",
    "\n",
    "    merge_varcope = pe.Node(fsl.Merge(dimension=\"t\"), name=\"merge_varcope\")\n",
    "    merge_varcope.inputs.in_files = varcopes\n",
    "\n",
    "    model = pe.Node(fsl.MultipleRegressDesign(), name=\"model\")\n",
    "    model.inputs.contrasts = contrasts\n",
    "    model.inputs.regressors = regressor\n",
    "    model.inputs.groups = [1 for _ in range(len(subjects))]\n",
    "\n",
    "\n",
    "    flameo = pe.Node(fsl.FLAMEO(\n",
    "        mask_file=mask_file,\n",
    "        run_mode=\"flame1\",\n",
    "    ), name=\"flameo\")\n",
    "\n",
    "    workflow.connect(model, 'design_mat', flameo, 'design_file')\n",
    "    workflow.connect(model, 'design_con', flameo, 't_con_file')\n",
    "    workflow.connect(merge_cope, 'merged_file', flameo, 'cope_file')\n",
    "    workflow.connect(merge_varcope, 'merged_file', flameo, 'var_cope_file')\n",
    "    workflow.connect(model, 'design_grp', flameo, 'cov_split_file')\n",
    "\n",
    "    smooth_estimate = pe.Node(fsl.SmoothEstimate(mask_file=mask_file), name=\"smooth_estimate\")\n",
    "\n",
    "    workflow.connect(flameo, 'zstats', smooth_estimate, 'zstat_file')\n",
    "\n",
    "    cluster = pe.Node(fsl.Cluster(), name=\"cluster\")\n",
    "    workflow.connect(smooth_estimate,'dlh', cluster, 'dlh')\n",
    "    workflow.connect(smooth_estimate, 'volume', cluster, 'volume')\n",
    "\n",
    "    cluster.inputs.connectivity = 26\n",
    "    cluster.inputs.threshold = 2.3\n",
    "    cluster.inputs.pthreshold = 0.05\n",
    "    cluster.inputs.out_threshold_file = True\n",
    "    cluster.inputs.out_index_file = True\n",
    "    cluster.inputs.out_localmax_txt_file = True\n",
    "    workflow.connect(flameo, \"zstats\", cluster, \"in_file\")\n",
    "\n",
    "    ztopval = pe.Node(fsl.ImageMaths(op_string=\"-ztop\", suffix=\"_pval\"), name=\"ztopval\")\n",
    "    workflow.connect(flameo, \"zstats\", ztopval, \"in_file\")\n",
    "\n",
    "    group_results_dir = Path(f\"group_results/scap/contrast{contrast_id}\")\n",
    "    group_results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    sinker = pe.Node(DataSink(), name=\"sinker\")\n",
    "    sinker.inputs.base_directory = str(group_results_dir.absolute())\n",
    "    sinker.inputs.substitutions = [('_cope_id', 'contrast'),('_maths_', '_reversed_')]\n",
    "\n",
    "    workflow.connect(flameo, 'zstats', sinker, 'stats')\n",
    "    workflow.connect(cluster, 'threshold_file', sinker, 'stats.@thr')\n",
    "    workflow.connect(cluster, 'index_file', sinker, 'stats.@index')\n",
    "    workflow.connect(cluster, 'localmax_txt_file', sinker, 'stats.@localmax')\n",
    "\n",
    "    zstats_reversed = pe.Node(fsl.maths.BinaryMaths(), name=\"zstats_reversed\")\n",
    "    zstats_reversed.inputs.operation = \"mul\"\n",
    "    zstats_reversed.inputs.operand_value = -1\n",
    "    workflow.connect(flameo, 'zstats', zstats_reversed, 'in_file')\n",
    "\n",
    "    cluster_reversed = cluster.clone(\"cluster_reversed\")\n",
    "    workflow.connect(smooth_estimate, 'dlh', cluster_reversed, 'dlh')\n",
    "    workflow.connect(smooth_estimate, 'volume', cluster_reversed, 'volume')\n",
    "    workflow.connect(zstats_reversed, 'out_file', cluster_reversed, 'in_file')\n",
    "\n",
    "    ztopval_reversed = ztopval.clone(\"ztopval_reversed\")\n",
    "    workflow.connect(zstats_reversed, 'out_file', ztopval_reversed, 'in_file')\n",
    "\n",
    "    workflow.connect(zstats_reversed, 'out_file', sinker, 'stats.@neg')\n",
    "    workflow.connect(cluster_reversed, 'threshold_file', sinker, 'stats.@neg_thr')\n",
    "    workflow.connect(cluster_reversed, 'index_file', sinker, 'stats.@neg_index')\n",
    "    workflow.connect(cluster_reversed, 'localmax_txt_file', sinker, 'stats.@neg_localmax')\n",
    "    return workflow\n",
    "\n",
    "\n",
    "for contrast_id in tqdm(SELECT_CONTRAST):\n",
    "    workflow = run_group_level_analysis(contrast_id)\n",
    "    workflow.run()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
