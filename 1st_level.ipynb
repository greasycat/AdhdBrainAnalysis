{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "895e4761",
   "metadata": {},
   "source": [
    "# First Level Analysis\n",
    "\n",
    "I start the investigation by conducting first-level analysis to obtain BOLD parameter estimates from individuals. There are many fMRI tools that can achieve this, and many of them have convenient graphical user interfaces (GUIs). Here, for reproducibility, we follow a similar pipeline as the original paper by using a combination of `fmriprep` and `nipype` with FSL and AFNI integrations. Both `fmriprep` and `nipype` are powerful, and the latter is highly flexible in terms of customizability compared to its GUI counterparts (such as FSL and AFNI).\n",
    "\n",
    "Nevertheless, the system environment is different, and we won't go into details on the setup process but will only provide a few caveats I encountered during setup:\n",
    "\n",
    "1. Using a Docker container like `neurodocker` is more reproducible and avoids many installation and version management problems. I used a non-containerized version just for the ease of coding and brain image inspections (provided by FSLeyes).\n",
    "\n",
    "2. The FSL and AFNI packages I installed on my Arch Linux distribution do not provide any PATH integration, so I needed to manually add the FSL executable folder to my `PATH` variable.\n",
    "\n",
    "3. I also installed the SPM standalone package but did not use SPM in any of the subsequent analysis. The SPM standalone requires extra setup code, which is provided below for anyone and my future self as a reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41fcdcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case we need to run SPM, here's the setup\n",
    "# %load setup_spm.py\n",
    "# from nipype.interfaces import spm\n",
    "\n",
    "# MATLAB_RUNTIME = '/home/rongfei/MATLAB/runtime/R2024b'\n",
    "# SPM_START_SCRIPT = '/home/rongfei/Builds/spm12/spm_standalone/run_spm25.sh'\n",
    "\n",
    "# MCR_COMMAND = f'{SPM_START_SCRIPT} {MATLAB_RUNTIME} script'\n",
    "# print(\"MCR_COMMAND: \", MCR_COMMAND)\n",
    "\n",
    "# spm.SPMCommand.set_mlab_paths(MCR_COMMAND, use_mcr=True)\n",
    "# print(spm.SPMCommand().version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2454fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check if FSL is runnable... ok\n"
     ]
    }
   ],
   "source": [
    "from nipype.interfaces import fsl\n",
    "\n",
    "# check FSL integration\n",
    "\n",
    "print(\"Check if FSL is runnable... \", end='')\n",
    "print('ok' if fsl.check_fsl() == 0 else 'not ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd02bc6",
   "metadata": {},
   "source": [
    "The following code block creates a customized `SubjectInfo` class that provides necessary brain image and event-related file information for later processing. Here I briefly list some of the class methods and their corresponding purposes:\n",
    "\n",
    "1. `__init__` constructor: Creates parent folder reference\n",
    "2. `filter_confound(list)`: Provides confounds dataframe produced by fMRIPrep\n",
    "3. `create_fsl_explanatory_variable_file`: Loads events from the event data file and converts into FSL 3-column format\n",
    "4. `create_fsl_evs_scap`: Separates different event conditions and applies the method above to create explanatory variables (EVs)\n",
    "\n",
    "Alternatively, we can use the `DataGrabber` and `IdentityInterface` classes provided by `nipype` to achieve the same functionalities and maintain design style consistency. However, I simply prefer writing my own class for more customization and better readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ed2f4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# Create a class to handle subject information\n",
    "class SubjectInfo:\n",
    "    def __init__(\n",
    "        self,\n",
    "        subject_id: str,\n",
    "        raw_data_dir: Path | None = None,\n",
    "        derivatives_dir: Path | None = None,\n",
    "        working_dir: Path | None = None,\n",
    "        results_dir: Path | None = None,\n",
    "    ):\n",
    "        if raw_data_dir is None:\n",
    "            raw_data_dir = Path(\"data\")\n",
    "        if derivatives_dir is None:\n",
    "            derivatives_dir = Path(\"derivatives\")\n",
    "        if working_dir is None:\n",
    "            working_dir = Path(\"working\")\n",
    "        if results_dir is None:\n",
    "            results_dir = Path(\"results\")\n",
    "\n",
    "        working_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        self.subject_id: str = subject_id\n",
    "        self.raw_data_dir: Path = raw_data_dir\n",
    "        self.derivatives_dir: Path = derivatives_dir\n",
    "        self.working_dir: Path = working_dir\n",
    "        self.results_dir: Path = results_dir\n",
    "\n",
    "    def get_confounds_file(self, task_name: str):\n",
    "        confounds_file = (\n",
    "            self.derivatives_dir\n",
    "            / self.subject_id\n",
    "            / \"func\"\n",
    "            / f\"{self.subject_id}_task-{task_name}_bold_confounds.tsv\"\n",
    "        )\n",
    "        if not confounds_file.exists():\n",
    "            raise FileNotFoundError(f\"Confounds file not found: {confounds_file}\")\n",
    "        return confounds_file\n",
    "\n",
    "    def get_event_file(self, task_name: str):\n",
    "        event_file = (\n",
    "            self.raw_data_dir\n",
    "            / self.subject_id\n",
    "            / \"func\"\n",
    "            / f\"{self.subject_id}_task-{task_name}_events.tsv\"\n",
    "        )\n",
    "        if not event_file.exists():\n",
    "            raise FileNotFoundError(f\"Event file not found: {event_file}\")\n",
    "        return event_file\n",
    "\n",
    "    def get_subject_id(self):\n",
    "        return self.subject_id\n",
    "\n",
    "    def filter_confounds(self, task_name: str, confounds_columns: list[str]):\n",
    "        confounds_file = self.get_confounds_file(task_name)\n",
    "        confounds_df = pd.read_csv(confounds_file, sep=\"\\t\", na_values=\"n/a\")\n",
    "        selected_columns = confounds_df[confounds_columns]\n",
    "\n",
    "        # map na to 0\n",
    "        selected_columns = selected_columns.fillna(0).astype(float)\n",
    "\n",
    "        selected_columns = selected_columns.round(3)\n",
    "\n",
    "        # get the file name without the extension and add filtered to the end before the extension\n",
    "        confounds_folder = self.working_dir / self.subject_id\n",
    "        confounds_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        confounds_file_name = (\n",
    "            confounds_folder\n",
    "            / f\"{self.subject_id}_task-{task_name}_confounds_filtered.tsv\"\n",
    "        )\n",
    "        selected_columns.to_csv(confounds_file_name, sep=\" \", header=False, index=False)\n",
    "        return confounds_file_name\n",
    "    \n",
    "    def get_anatomical_file(self):\n",
    "        anat_file = (\n",
    "            self.derivatives_dir\n",
    "            / self.subject_id\n",
    "            / \"anat\"\n",
    "            / f\"{self.subject_id}_T1w_space-MNI152NLin2009cAsym_preproc.nii.gz\"\n",
    "        )\n",
    "        if not anat_file.exists():\n",
    "            raise FileNotFoundError(f\"Anatomical file not found: {anat_file}\")\n",
    "        return anat_file\n",
    "\n",
    "    def get_standard_brain_mask(self, task_name: str):\n",
    "        mask_file = (\n",
    "            self.derivatives_dir\n",
    "            / self.subject_id\n",
    "            / \"func\"\n",
    "            / f\"{self.subject_id}_task-{task_name}_bold_space-MNI152NLin2009cAsym_brainmask.nii.gz\"\n",
    "        )\n",
    "        if not mask_file.exists():\n",
    "            raise FileNotFoundError(f\"Mask file not found: {mask_file}\")\n",
    "        return mask_file\n",
    "\n",
    "    def get_transformed_brain_mask(self, task_name: str):\n",
    "        mask_file = (\n",
    "            self.derivatives_dir\n",
    "            / self.subject_id\n",
    "            / \"func\"\n",
    "            / f\"{self.subject_id}_task-{task_name}_bold_space-MNI152NLin2009cAsym_brainmask_processed.nii.gz\"\n",
    "        )\n",
    "        return mask_file\n",
    "\n",
    "    def get_processed_bold_file(self, task_name: str):\n",
    "        bold_file = (\n",
    "            self.derivatives_dir\n",
    "            / self.subject_id\n",
    "            / \"func\"\n",
    "            / f\"{self.subject_id}_task-{task_name}_bold_space-MNI152NLin2009cAsym_preproc.nii.gz\"\n",
    "        )\n",
    "        if not bold_file.exists():\n",
    "            raise FileNotFoundError(f\"Bold file not found: {bold_file}\")\n",
    "        return bold_file\n",
    "\n",
    "    def get_smoothed_bold_file(self, task_name: str):\n",
    "        bold_file = (\n",
    "            self.derivatives_dir\n",
    "            / self.subject_id\n",
    "            / \"func\"\n",
    "            / f\"{self.subject_id}_task-{task_name}_bold_space-MNI152NLin2009cAsym_preproc_smoothed.nii.gz\"\n",
    "        )\n",
    "        return bold_file\n",
    "\n",
    "    def get_events(self, task_name: str):\n",
    "        events_file = self.get_event_file(task_name)\n",
    "        if not events_file.exists():\n",
    "            raise FileNotFoundError(f\"Events file not found: {events_file}\")\n",
    "\n",
    "        events_df = pd.read_csv(events_file, sep=\"\\t\", na_values=\"n/a\")\n",
    "        return events_df\n",
    "    \n",
    "    def get_working_task_dir(self, task_name: str):\n",
    "        task_dir = self.working_dir / self.subject_id / task_name\n",
    "        task_dir.mkdir(parents=True, exist_ok=True)\n",
    "        return task_dir\n",
    "    \n",
    "    def get_output_dir(self):\n",
    "        task_dir = self.results_dir / self.subject_id \n",
    "        task_dir.mkdir(parents=True, exist_ok=True)\n",
    "        return task_dir\n",
    "\n",
    "    def create_fsl_explanatory_variable_file(\n",
    "        self,\n",
    "        task_name: str,\n",
    "        events_df: pd.DataFrame,\n",
    "        ev_name: str,\n",
    "        reference_duration_column: str | None = None,\n",
    "        reference_amplitude_column: str | None = None,\n",
    "        fixed_duration: float = 1.0,\n",
    "        fixed_amplitude: float = 1.0,\n",
    "    ):\n",
    "        events_df = events_df.dropna(subset=[\"onset\"])\n",
    "        if events_df.empty:\n",
    "            warnings.warn(f\"No events found in the events file for {ev_name}\")\n",
    "            \n",
    "\n",
    "\n",
    "        onsets = events_df[\"onset\"].to_list()\n",
    "        if reference_duration_column is not None: \n",
    "            # if we have a reference duration column\n",
    "            durations = events_df[reference_duration_column].to_list()\n",
    "        else: \n",
    "            # if we don't have a reference duration column, use a fixed duration\n",
    "            durations = [fixed_duration] * len(onsets)\n",
    "\n",
    "        if reference_amplitude_column is not None:\n",
    "            # center the amplitude around 0\n",
    "            amplitudes = events_df[reference_amplitude_column]\n",
    "            amplitudes = (\n",
    "                amplitudes.apply(lambda x: x - np.mean(amplitudes)).round(3).to_list()\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            amplitudes = [fixed_amplitude] * len(onsets)\n",
    "\n",
    "        fsl_explanatory_variable_df = pd.DataFrame(\n",
    "            {\"0\": onsets, \"1\": durations, \"2\": amplitudes}\n",
    "        )\n",
    "\n",
    "        ev_file_folder = self.working_dir / self.subject_id / task_name \n",
    "        ev_file_folder.mkdir(parents=True, exist_ok=True)\n",
    "        ev_file_path = ev_file_folder / f\"{ev_name}.txt\"\n",
    "\n",
    "        fsl_explanatory_variable_df.to_csv(\n",
    "            ev_file_path,\n",
    "            sep=\"\\t\",\n",
    "            header=False,\n",
    "            index=False,\n",
    "        )\n",
    "\n",
    "        return ev_file_path\n",
    "\n",
    "\n",
    "    def create_fsl_evs_scap(self):\n",
    "        events_df = self.get_events(\"scap\")\n",
    "\n",
    "        # separate correct and incorrect events\n",
    "        correct_events = events_df[events_df[\"ResponseAccuracy\"] == \"CORRECT\"]\n",
    "        incorrect_events = events_df[events_df[\"ResponseAccuracy\"] == \"INCORRECT\"]\n",
    "\n",
    "        evs = []\n",
    "        n_regressors = 25 # number of regressors as stated in the original paper\n",
    "\n",
    "        # initialize orthogonalization dictionary to remove reaction time correlation PS: a weird FSL format\n",
    "        orthogonalization = {x: {y:0 for y in range(1,n_regressors+1)} for x in range(1,n_regressors+1)}\n",
    "\n",
    "        # load and delay levels as stated in the original paper\n",
    "        load_levels = [1, 3, 5, 7]\n",
    "        delay_levels = [1.5, 3.0, 4.5]\n",
    "\n",
    "        for load_level in load_levels:\n",
    "            for delay_level in delay_levels:\n",
    "                condition_mask = (correct_events[\"Load\"] == load_level) & (\n",
    "                    correct_events[\"Delay\"] == delay_level\n",
    "                )\n",
    "                condition_events = correct_events[condition_mask]\n",
    "\n",
    "                if condition_events.empty:\n",
    "                    warnings.warn(\n",
    "                        f\"No events found for load {load_level} and delay {delay_level}\"\n",
    "                    )\n",
    "                \n",
    "                # condition_events[\"onset\"] = condition_events[\"onset\"] + delay_level\n",
    "                condition_events.loc[:, \"onset\"] = condition_events[\"onset\"] + delay_level\n",
    "\n",
    "                fixed_ev = self.create_fsl_explanatory_variable_file(\n",
    "                    \"scap\",\n",
    "                    condition_events,\n",
    "                    f\"LOAD{load_level}_DELAY{delay_level}\",\n",
    "                    fixed_duration=3.0,\n",
    "                    fixed_amplitude=1.0,\n",
    "                )\n",
    "\n",
    "                reaction_time_ev = self.create_fsl_explanatory_variable_file(\n",
    "                    \"scap\",\n",
    "                    condition_events,\n",
    "                    f\"LOAD{load_level}_DELAY{delay_level}_rt\",\n",
    "                    reference_duration_column=\"ReactionTime\",\n",
    "                    fixed_amplitude=1.0,\n",
    "                )\n",
    "\n",
    "                evs.append(fixed_ev)\n",
    "                fix_index = len(evs) \n",
    "\n",
    "                evs.append(reaction_time_ev)\n",
    "                reaction_time_index = len(evs)\n",
    "                orthogonalization[reaction_time_index][fix_index] = 1\n",
    "                orthogonalization[reaction_time_index][0] = 1\n",
    "\n",
    "\n",
    "        \n",
    "        if not incorrect_events.empty:\n",
    "            error_ev = self.create_fsl_explanatory_variable_file(\n",
    "                \"scap\",\n",
    "                incorrect_events,\n",
    "                \"Error\",\n",
    "                fixed_duration=3.0,\n",
    "                fixed_amplitude=1.0,\n",
    "            )\n",
    "            evs.append(error_ev)\n",
    "        \n",
    "\n",
    "        valid_ev_files = []\n",
    "        for ev_file in evs:\n",
    "            if os.path.exists(ev_file) and os.path.getsize(ev_file) > 0:\n",
    "                valid_ev_files.append(ev_file.absolute().as_posix())\n",
    "            else:\n",
    "                print(f\"Warning: Empty or missing EV file: {ev_file}\")\n",
    "        \n",
    "        # Validation\n",
    "        expected_regressors = len(load_levels) * len(delay_levels) * 2 + 1  # 25\n",
    "        if len(valid_ev_files) != expected_regressors:\n",
    "            print(f\"Warning: Expected {expected_regressors} regressors, got {len(valid_ev_files)}\")\n",
    "\n",
    "        \n",
    "        return {\n",
    "            'event_files': valid_ev_files,\n",
    "            'orthogonalization': orthogonalization\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef2177f",
   "metadata": {},
   "source": [
    "Next, I create the 20+20 (bidirectional) contrasts stated by the original paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6018f37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_scap_contrast():\n",
    "    contrasts = []\n",
    "\n",
    "    # All conditions\n",
    "    contrasts += [\n",
    "        (\"All\",\"T\",[\"LOAD1_DELAY1.5\",\"LOAD1_DELAY3.0\",\"LOAD1_DELAY4.5\",\"LOAD3_DELAY1.5\",\"LOAD3_DELAY3.0\",\"LOAD3_DELAY4.5\",\"LOAD5_DELAY1.5\",\"LOAD5_DELAY3.0\",\"LOAD5_DELAY4.5\",\"LOAD7_DELAY1.5\",\"LOAD7_DELAY3.0\",\"LOAD7_DELAY4.5\"],\n",
    "            [1] * 12,\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    contrasts += [\n",
    "        (\"All_rt\",\"T\",[\"LOAD1_DELAY1.5_rt\",\"LOAD1_DELAY3.0_rt\",\"LOAD1_DELAY4.5_rt\",\"LOAD3_DELAY1.5_rt\",\"LOAD3_DELAY3.0_rt\",\"LOAD3_DELAY4.5_rt\",\"LOAD5_DELAY1.5_rt\",\"LOAD5_DELAY3.0_rt\",\"LOAD5_DELAY4.5_rt\",\"LOAD7_DELAY1.5_rt\",\"LOAD7_DELAY3.0_rt\",\"LOAD7_DELAY4.5_rt\"],\n",
    "            [1] * 12,\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # # Load effects\n",
    "    contrasts += [\n",
    "        (\"Load1\",\"T\", [\"LOAD1_DELAY1.5\",\"LOAD1_DELAY3.0\",\"LOAD1_DELAY4.5\"], [1] * 3)\n",
    "    ]\n",
    "    contrasts += [\n",
    "        (\"Load3\",\"T\", [\"LOAD3_DELAY1.5\",\"LOAD3_DELAY3.0\",\"LOAD3_DELAY4.5\"], [1] * 3)\n",
    "    ]\n",
    "    contrasts += [\n",
    "        (\"Load5\",\"T\", [\"LOAD5_DELAY1.5\",\"LOAD5_DELAY3.0\",\"LOAD5_DELAY4.5\"], [1] * 3)\n",
    "    ]\n",
    "    contrasts += [\n",
    "        (\"Load7\",\"T\", [\"LOAD7_DELAY1.5\",\"LOAD7_DELAY3.0\",\"LOAD7_DELAY4.5\"], [1] * 3)\n",
    "    ]\n",
    "\n",
    "    # # Delay effects\n",
    "    contrasts += [\n",
    "        (\"Delay1.5\",\"T\",[\"LOAD1_DELAY1.5\",\"LOAD3_DELAY1.5\",\"LOAD5_DELAY1.5\",\"LOAD7_DELAY1.5\"],\n",
    "            [1] * 4,\n",
    "        )\n",
    "    ]\n",
    "    contrasts += [\n",
    "        (\"Delay3\",\"T\",[\"LOAD1_DELAY3.0\",\"LOAD3_DELAY3.0\",\"LOAD5_DELAY3.0\",\"LOAD7_DELAY3.0\"],\n",
    "            [1] * 4,\n",
    "        )\n",
    "    ]\n",
    "    contrasts += [\n",
    "        (\"Delay4.5\",\"T\",[\"LOAD1_DELAY4.5\",\"LOAD3_DELAY4.5\",\"LOAD5_DELAY4.5\",\"LOAD7_DELAY4.5\"],\n",
    "            [1] * 4,\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Parametric contrasts\n",
    "    contrasts += [\n",
    "        (\"LinearUp_load\",\"T\",[\"LOAD1_DELAY1.5\",\"LOAD1_DELAY3.0\",\"LOAD1_DELAY4.5\",\"LOAD3_DELAY1.5\",\"LOAD3_DELAY3.0\",\"LOAD3_DELAY4.5\",\"LOAD5_DELAY1.5\",\"LOAD5_DELAY3.0\",\"LOAD5_DELAY4.5\",\"LOAD7_DELAY1.5\",\"LOAD7_DELAY3.0\",\"LOAD7_DELAY4.5\",            ],\n",
    "            [-3] * 3 + [-1] * 3 + [1] * 3 + [3] * 3,\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    contrasts += [\n",
    "        (\"LinearUp_delay\",\"T\",[\"LOAD1_DELAY1.5\",\"LOAD1_DELAY3.0\",\"LOAD1_DELAY4.5\",\"LOAD3_DELAY1.5\",\"LOAD3_DELAY3.0\",\"LOAD3_DELAY4.5\",\"LOAD5_DELAY1.5\",\"LOAD5_DELAY3.0\",\"LOAD5_DELAY4.5\",\"LOAD7_DELAY1.5\",\"LOAD7_DELAY3.0\",\"LOAD7_DELAY4.5\",            ],\n",
    "            [-1, 0, 1] * 4,\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Pairwise load comparisons\n",
    "    contrasts += [\n",
    "        (\"Load3-load1\",\"T\",[\"LOAD1_DELAY1.5\",\"LOAD1_DELAY3.0\",\"LOAD1_DELAY4.5\",\"LOAD3_DELAY1.5\",\"LOAD3_DELAY3.0\",\"LOAD3_DELAY4.5\",            ],\n",
    "            [-1, -1, -1, 1, 1, 1],\n",
    "        )\n",
    "    ]\n",
    "    contrasts += [\n",
    "        (\"Load5-load1\",\"T\",[\"LOAD1_DELAY1.5\",\"LOAD1_DELAY3.0\",\"LOAD1_DELAY4.5\",\"LOAD5_DELAY1.5\",\"LOAD5_DELAY3.0\",\"LOAD5_DELAY4.5\",            ],\n",
    "            [-1, -1, -1, 1, 1, 1],\n",
    "        )\n",
    "    ]\n",
    "    contrasts += [\n",
    "        (\"Load7-load1\",\"T\",[\"LOAD1_DELAY1.5\",\"LOAD1_DELAY3.0\",\"LOAD1_DELAY4.5\",\"LOAD7_DELAY1.5\",\"LOAD7_DELAY3.0\",\"LOAD7_DELAY4.5\",            ],\n",
    "            [-1, -1, -1, 1, 1, 1],\n",
    "        )\n",
    "    ]\n",
    "    contrasts += [\n",
    "        (\"Load5-load3\",\"T\",[\"LOAD3_DELAY1.5\",\"LOAD3_DELAY3.0\",\"LOAD3_DELAY4.5\",\"LOAD5_DELAY1.5\",\"LOAD5_DELAY3.0\",\"LOAD5_DELAY4.5\",],\n",
    "            [-1, -1, -1, 1, 1, 1],\n",
    "        )\n",
    "    ]\n",
    "    contrasts += [\n",
    "        (\"Load7-load3\",\"T\",[\"LOAD3_DELAY1.5\",\"LOAD3_DELAY3.0\",\"LOAD3_DELAY4.5\",\"LOAD7_DELAY1.5\",\"LOAD7_DELAY3.0\",\"LOAD7_DELAY4.5\",],\n",
    "            [-1, -1, -1, 1, 1, 1],\n",
    "        )\n",
    "    ]\n",
    "    contrasts += [\n",
    "        (\"Load7-load5\",\"T\",[\"LOAD5_DELAY1.5\",\"LOAD5_DELAY3.0\",\"LOAD5_DELAY4.5\",\"LOAD7_DELAY1.5\",\"LOAD7_DELAY3.0\",\"LOAD7_DELAY4.5\",],\n",
    "            [-1, -1, -1, 1, 1, 1],\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Pairwise delay comparisons\n",
    "    contrasts += [\n",
    "        (\"Delay4_5-delay1_5\",\"T\",[\"LOAD1_DELAY1.5\",\"LOAD3_DELAY1.5\",\"LOAD5_DELAY1.5\",\"LOAD7_DELAY1.5\",\"LOAD1_DELAY4.5\",\"LOAD3_DELAY4.5\",\"LOAD5_DELAY4.5\",\"LOAD7_DELAY4.5\",            ],\n",
    "            [-1, -1, -1, -1, 1, 1, 1, 1],\n",
    "        )\n",
    "    ]\n",
    "    contrasts += [\n",
    "        (\"Delay3-delay1_5\",\"T\",[\"LOAD1_DELAY1.5\",\"LOAD3_DELAY1.5\",\"LOAD5_DELAY1.5\",\"LOAD7_DELAY1.5\",\"LOAD1_DELAY3.0\",\"LOAD3_DELAY3.0\",\"LOAD5_DELAY3.0\",\"LOAD7_DELAY3.0\",            ],\n",
    "            [-1, -1, -1, -1, 1, 1, 1, 1],\n",
    "        )\n",
    "    ]\n",
    "    contrasts += [\n",
    "        (\"Delay4_5-delay3\",\"T\",[\"LOAD1_DELAY3.0\",\"LOAD3_DELAY3.0\",\"LOAD5_DELAY3.0\",\"LOAD7_DELAY3.0\",\"LOAD1_DELAY4.5\",\"LOAD3_DELAY4.5\",\"LOAD5_DELAY4.5\",\"LOAD7_DELAY4.5\",            ],\n",
    "            [-1, -1, -1, -1, 1, 1, 1, 1],\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Create bidirectional contrasts (positive and negative)\n",
    "    bidirectional_contrasts = []\n",
    "    for name, contrast_type, conditions, weights in contrasts:\n",
    "        # Add original\n",
    "        bidirectional_contrasts.append((name, contrast_type, conditions, weights))\n",
    "\n",
    "        if\"-\" in name:\n",
    "            # Reverse:\"A-B\" becomes\"B-A\"\n",
    "            parts = name.split(\"-\", 1)\n",
    "            neg_name = f\"{parts[1]}-{parts[0]}\"\n",
    "        else:\n",
    "            # Add neg_ prefix\n",
    "            neg_name = f\"neg_{name}\"\n",
    "\n",
    "        bidirectional_contrasts.append(\n",
    "            (neg_name, contrast_type, conditions, [-w for w in weights])\n",
    "        )\n",
    "\n",
    "    contrasts = bidirectional_contrasts\n",
    "    return contrasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3857d299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: All\n",
      "2: neg_All\n",
      "3: All_rt\n",
      "4: neg_All_rt\n",
      "5: Load1\n",
      "6: neg_Load1\n",
      "7: Load3\n",
      "8: neg_Load3\n",
      "9: Load5\n",
      "10: neg_Load5\n",
      "11: Load7\n",
      "12: neg_Load7\n",
      "13: Delay1.5\n",
      "14: neg_Delay1.5\n",
      "15: Delay3\n",
      "16: neg_Delay3\n",
      "17: Delay4.5\n",
      "18: neg_Delay4.5\n",
      "19: LinearUp_load\n",
      "20: neg_LinearUp_load\n",
      "21: LinearUp_delay\n",
      "22: neg_LinearUp_delay\n",
      "23: Load3-load1\n",
      "24: load1-Load3\n",
      "25: Load5-load1\n",
      "26: load1-Load5\n",
      "27: Load7-load1\n",
      "28: load1-Load7\n",
      "29: Load5-load3\n",
      "30: load3-Load5\n",
      "31: Load7-load3\n",
      "32: load3-Load7\n",
      "33: Load7-load5\n",
      "34: load5-Load7\n",
      "35: Delay4_5-delay1_5\n",
      "36: delay1_5-Delay4_5\n",
      "37: Delay3-delay1_5\n",
      "38: delay1_5-Delay3\n",
      "39: Delay4_5-delay3\n",
      "40: delay3-Delay4_5\n"
     ]
    }
   ],
   "source": [
    "contrasts = create_scap_contrast()\n",
    "for i, contrast in enumerate(contrasts):\n",
    "    print(f\"{i+1}: {contrast[0]}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e9976a",
   "metadata": {},
   "source": [
    "Now comes the actual workflow\n",
    "\n",
    "![workflow](figs/first_level.png)\n",
    "\n",
    "- The workflow applies the brain mask to specify the valid brain region, then applies AFNI 3D spatial smoothing to enhance the signal.\n",
    "- Next, the selected confounds (motion parameters) from fMRIPrep are passed to the model as `realignment_parameter` for additional artifact removal. The orthogonalization dictionary (to remove covariance from reaction time regressors) is also passed for accurate modeling.\n",
    "- The model uses the double gamma derivative HRF as the paper suggests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94dd1dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a workflow function\n",
    "\n",
    "import nipype.pipeline.engine as pe\n",
    "import nipype.interfaces.fsl as fsl\n",
    "import nipype.interfaces.afni as afni\n",
    "from nipype.algorithms.modelgen import SpecifyModel\n",
    "import shutil\n",
    "\n",
    "\n",
    "def subject_level_modeling(subject_info: SubjectInfo, task_name='scap'):\n",
    "    workflow = pe.Workflow(name=\"first_level\")\n",
    "    \n",
    "    # Define the input nodes to accept inputs of the following:\n",
    "    # - functional image\n",
    "    # - brain mask\n",
    "    # - confounds file\n",
    "\n",
    "\n",
    "    in_file = subject_info.get_processed_bold_file(task_name).absolute().as_posix()\n",
    "    mask_file = subject_info.get_standard_brain_mask(task_name).absolute().as_posix()\n",
    "    out_file = subject_info.get_processed_bold_file(task_name).absolute().as_posix()\n",
    "\n",
    "    print(in_file)\n",
    "    print(mask_file)\n",
    "    print(out_file)\n",
    "    apply_mask = pe.Node(\n",
    "        fsl.maths.ApplyMask(\n",
    "            in_file=in_file,\n",
    "            mask_file=mask_file,\n",
    "            out_file=out_file,\n",
    "        ),\n",
    "        name=\"apply_mask\",\n",
    "    )\n",
    "\n",
    "    mask_file = subject_info.get_standard_brain_mask(task_name).absolute().as_posix()\n",
    "    out_file = subject_info.get_smoothed_bold_file(task_name).absolute().as_posix()\n",
    "\n",
    "    apply_smooth = pe.Node(\n",
    "        afni.BlurInMask(\n",
    "            mask=mask_file,\n",
    "            out_file=out_file,\n",
    "            fwhm=5.0, # Suggested by original paper\n",
    "        ),\n",
    "        name=\"apply_smooth\",\n",
    "    )\n",
    "\n",
    "    # Get confounds\n",
    "\n",
    "    confounds_file = subject_info.filter_confounds(task_name, ['stdDVARS', 'non-stdDVARS', 'vx-wisestdDVARS',\n",
    "                                 'FramewiseDisplacement', 'X', 'Y', 'Z', 'RotX', 'RotY', 'RotZ'])\n",
    "    confounds_file = confounds_file.absolute().as_posix()\n",
    "\n",
    "    # Get regressors (as event files) and orthogonality matrix\n",
    "    event_files = subject_info.create_fsl_evs_scap()['event_files']\n",
    "    orthogonalization = subject_info.create_fsl_evs_scap()['orthogonalization']\n",
    "\n",
    "    model_specification = pe.Node(\n",
    "        SpecifyModel(\n",
    "            event_files=event_files,\n",
    "            realignment_parameters=confounds_file,\n",
    "            input_units='secs',\n",
    "            time_repetition=2.0,\n",
    "            high_pass_filter_cutoff=100,\n",
    "        ),\n",
    "        name=\"model_specification\",\n",
    "    )\n",
    "\n",
    "    l1design = pe.Node(\n",
    "        fsl.Level1Design(\n",
    "            contrasts=create_scap_contrast(),\n",
    "            orthogonalization=orthogonalization,\n",
    "            interscan_interval=2.0,\n",
    "            bases={'dgamma': {'derivs': True}},\n",
    "            model_serial_correlations=True,\n",
    "        ),\n",
    "        name=\"l1design\",\n",
    "    )\n",
    "\n",
    "    l1feat = pe.Node(\n",
    "        fsl.FEATModel(),\n",
    "        name=\"l1feat\",\n",
    "    )\n",
    "\n",
    "    l1estimate = pe.Node(\n",
    "        fsl.FEAT(),\n",
    "        name=\"l1estimate\",\n",
    "    )\n",
    "\n",
    "    workflow.base_dir = str(subject_info.get_working_task_dir(task_name))\n",
    "    workflow.connect([\n",
    "        (apply_mask, apply_smooth, [('out_file', 'in_file')]),\n",
    "        (apply_smooth, model_specification, [('out_file', 'functional_runs')]),\n",
    "        (model_specification, l1design, [('session_info', 'session_info')]),\n",
    "        (l1design, l1feat, [('fsf_files', 'fsf_file'), ('ev_files', 'ev_files')]),\n",
    "        (l1design, l1estimate, [('fsf_files', 'fsf_file')]), # Note the plural difference\n",
    "    ])\n",
    "\n",
    "    # workflow.write_graph(graph2use='colored', format='png', simple_form=True)\n",
    "    return workflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "882d4504",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_workflow(workflow, subject_info, task_name, remove_previous=False):\n",
    "    workflow.run('MultiProc')\n",
    "\n",
    "    working_feat_dir = subject_info.get_working_task_dir(task_name) / 'first_level' / 'l1estimate'/ 'run0.feat'\n",
    "\n",
    "    results_feat_dir = subject_info.get_output_dir() / 'scap.feat'\n",
    "\n",
    "    results_feat_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if remove_previous:\n",
    "        if results_feat_dir.exists():\n",
    "            shutil.rmtree(results_feat_dir)\n",
    "\n",
    "    # move working_feat_dir to results_feat_dir\n",
    "    shutil.move(working_feat_dir, results_feat_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65af7553",
   "metadata": {},
   "source": [
    "The following block loads the sampled subjects' ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f47a27be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "# read control_subjects_ids.txt and adhd_subjects_ids.txt into lists\n",
    "control_subjects_ids = pd.read_csv('control_subjects_ids.txt', header=None).iloc[:, 0].tolist()\n",
    "adhd_subjects_ids = pd.read_csv('adhd_subjects_ids.txt', header=None).iloc[:, 0].tolist()\n",
    "\n",
    "assert len(control_subjects_ids) == len(adhd_subjects_ids)\n",
    "\n",
    "ids = []\n",
    "for i in range(len(control_subjects_ids)):\n",
    "    ids.append(control_subjects_ids[i])\n",
    "    ids.append(adhd_subjects_ids[i])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "118084f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rongfei/WorkSpace/consortium/derivatives/sub-11106/func/sub-11106_task-scap_bold_space-MNI152NLin2009cAsym_preproc.nii.gz\n",
      "/home/rongfei/WorkSpace/consortium/derivatives/sub-11106/func/sub-11106_task-scap_bold_space-MNI152NLin2009cAsym_brainmask.nii.gz\n",
      "/home/rongfei/WorkSpace/consortium/derivatives/sub-11106/func/sub-11106_task-scap_bold_space-MNI152NLin2009cAsym_preproc.nii.gz\n",
      "250921-14:48:56,298 nipype.workflow INFO:\n",
      "\t Workflow first_level settings: ['check', 'execution', 'logging', 'monitoring']\n",
      "250921-14:48:56,302 nipype.workflow INFO:\n",
      "\t Running in parallel.\n",
      "250921-14:48:56,303 nipype.workflow INFO:\n",
      "\t [MultiProc] Running 0 tasks, and 1 jobs ready. Free memory (GB): 54.38/54.38, Free processors: 24/24, Free GPU slot:1/1.\n",
      "250921-14:48:56,455 nipype.workflow INFO:\n",
      "\t [Node] Outdated cache found for \"first_level.apply_mask\".\n",
      "250921-14:48:56,455 nipype.workflow INFO:\n",
      "\t [Node] Outdated cache found for \"first_level.apply_mask\".\n",
      "250921-14:48:56,579 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"first_level.apply_mask\" in \"/home/rongfei/WorkSpace/consortium/working/sub-11106/scap/first_level/apply_mask\".\n",
      "250921-14:48:56,581 nipype.workflow INFO:\n",
      "\t [Node] Outdated cache found for \"first_level.apply_mask\".\n",
      "250921-14:48:56,583 nipype.workflow INFO:\n",
      "\t [Node] Executing \"apply_mask\" <nipype.interfaces.fsl.maths.ApplyMask>\n",
      "250921-14:48:58,304 nipype.workflow INFO:\n",
      "\t [MultiProc] Running 1 tasks, and 0 jobs ready. Free memory (GB): 54.18/54.38, Free processors: 23/24, Free GPU slot:1/1.\n",
      "                     Currently running:\n",
      "                       * first_level.apply_mask\n",
      "250921-14:48:59,835 nipype.workflow INFO:\n",
      "\t [Node] Finished \"apply_mask\", elapsed time 3.25092s.\n",
      "250921-14:49:00,304 nipype.workflow INFO:\n",
      "\t [Job 0] Completed (first_level.apply_mask).\n",
      "250921-14:49:00,305 nipype.workflow INFO:\n",
      "\t [MultiProc] Running 0 tasks, and 1 jobs ready. Free memory (GB): 54.38/54.38, Free processors: 24/24, Free GPU slot:1/1.\n",
      "250921-14:49:00,458 nipype.workflow INFO:\n",
      "\t [Node] Outdated cache found for \"first_level.apply_smooth\".\n",
      "250921-14:49:00,458 nipype.workflow INFO:\n",
      "\t [Node] Outdated cache found for \"first_level.apply_smooth\".\n",
      "250921-14:49:00,460 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"first_level.apply_smooth\" in \"/home/rongfei/WorkSpace/consortium/working/sub-11106/scap/first_level/apply_smooth\".\n",
      "250921-14:49:00,462 nipype.workflow INFO:\n",
      "\t [Node] Outdated cache found for \"first_level.apply_smooth\".\n",
      "250921-14:49:00,464 nipype.workflow INFO:\n",
      "\t [Node] Executing \"apply_smooth\" <nipype.interfaces.afni.preprocess.BlurInMask>\n",
      "250921-14:49:01,387 nipype.workflow INFO:\n",
      "\t [Node] Finished \"apply_smooth\", elapsed time 0.921157s.\n",
      "250921-14:49:02,304 nipype.workflow INFO:\n",
      "\t [Job 1] Completed (first_level.apply_smooth).\n",
      "250921-14:49:02,305 nipype.workflow INFO:\n",
      "\t [MultiProc] Running 0 tasks, and 1 jobs ready. Free memory (GB): 54.38/54.38, Free processors: 24/24, Free GPU slot:1/1.\n",
      "250921-14:49:02,431 nipype.workflow INFO:\n",
      "\t [Node] Outdated cache found for \"first_level.model_specification\".\n",
      "250921-14:49:02,431 nipype.workflow INFO:\n",
      "\t [Node] Outdated cache found for \"first_level.model_specification\".\n",
      "250921-14:49:02,434 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"first_level.model_specification\" in \"/home/rongfei/WorkSpace/consortium/working/sub-11106/scap/first_level/model_specification\".\n",
      "250921-14:49:02,436 nipype.workflow INFO:\n",
      "\t [Node] Outdated cache found for \"first_level.model_specification\".\n",
      "250921-14:49:02,438 nipype.workflow INFO:\n",
      "\t [Node] Executing \"model_specification\" <nipype.algorithms.modelgen.SpecifyModel>\n",
      "250921-14:49:02,441 nipype.workflow INFO:\n",
      "\t [Node] Finished \"model_specification\", elapsed time 0.002108s.\n",
      "250921-14:49:04,304 nipype.workflow INFO:\n",
      "\t [Job 2] Completed (first_level.model_specification).\n",
      "250921-14:49:04,305 nipype.workflow INFO:\n",
      "\t [MultiProc] Running 0 tasks, and 1 jobs ready. Free memory (GB): 54.38/54.38, Free processors: 24/24, Free GPU slot:1/1.\n",
      "250921-14:49:04,468 nipype.workflow INFO:\n",
      "\t [Job 3] Cached (first_level.l1design).\n",
      "250921-14:49:06,304 nipype.workflow INFO:\n",
      "\t [MultiProc] Running 0 tasks, and 2 jobs ready. Free memory (GB): 54.38/54.38, Free processors: 24/24, Free GPU slot:1/1.\n",
      "250921-14:49:06,453 nipype.workflow INFO:\n",
      "\t [Job 4] Cached (first_level.l1feat).\n",
      "250921-14:49:06,456 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"first_level.l1estimate\" in \"/home/rongfei/WorkSpace/consortium/working/sub-11106/scap/first_level/l1estimate\".\n",
      "250921-14:49:06,464 nipype.workflow INFO:\n",
      "\t [Node] Executing \"l1estimate\" <nipype.interfaces.fsl.model.FEAT>\n",
      "250921-14:49:08,304 nipype.workflow INFO:\n",
      "\t [MultiProc] Running 1 tasks, and 0 jobs ready. Free memory (GB): 54.18/54.38, Free processors: 23/24, Free GPU slot:1/1.\n",
      "                     Currently running:\n",
      "                       * first_level.l1estimate\n",
      "250921-14:51:51,357 nipype.workflow INFO:\n",
      "\t [Node] Finished \"l1estimate\", elapsed time 164.891376s.\n",
      "250921-14:51:52,318 nipype.workflow INFO:\n",
      "\t [Job 5] Completed (first_level.l1estimate).\n",
      "250921-14:51:52,319 nipype.workflow INFO:\n",
      "\t [MultiProc] Running 0 tasks, and 0 jobs ready. Free memory (GB): 54.38/54.38, Free processors: 24/24, Free GPU slot:1/1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [02:58<08:54, 178.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rongfei/WorkSpace/consortium/derivatives/sub-11044/func/sub-11044_task-scap_bold_space-MNI152NLin2009cAsym_preproc.nii.gz\n",
      "/home/rongfei/WorkSpace/consortium/derivatives/sub-11044/func/sub-11044_task-scap_bold_space-MNI152NLin2009cAsym_brainmask.nii.gz\n",
      "/home/rongfei/WorkSpace/consortium/derivatives/sub-11044/func/sub-11044_task-scap_bold_space-MNI152NLin2009cAsym_preproc.nii.gz\n",
      "250921-14:51:54,462 nipype.workflow INFO:\n",
      "\t Workflow first_level settings: ['check', 'execution', 'logging', 'monitoring']\n",
      "250921-14:51:54,464 nipype.workflow INFO:\n",
      "\t Running in parallel.\n",
      "250921-14:51:54,465 nipype.workflow INFO:\n",
      "\t [MultiProc] Running 0 tasks, and 1 jobs ready. Free memory (GB): 54.38/54.38, Free processors: 24/24, Free GPU slot:1/1.\n",
      "250921-14:51:54,762 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"first_level.apply_mask\" in \"/home/rongfei/WorkSpace/consortium/working/sub-11044/scap/first_level/apply_mask\".\n",
      "250921-14:51:54,765 nipype.workflow INFO:\n",
      "\t [Node] Executing \"apply_mask\" <nipype.interfaces.fsl.maths.ApplyMask>\n",
      "250921-14:51:56,467 nipype.workflow INFO:\n",
      "\t [MultiProc] Running 1 tasks, and 0 jobs ready. Free memory (GB): 54.18/54.38, Free processors: 23/24, Free GPU slot:1/1.\n",
      "                     Currently running:\n",
      "                       * first_level.apply_mask\n",
      "250921-14:51:58,1 nipype.workflow INFO:\n",
      "\t [Node] Finished \"apply_mask\", elapsed time 3.234883s.\n",
      "250921-14:51:58,466 nipype.workflow INFO:\n",
      "\t [Job 0] Completed (first_level.apply_mask).\n",
      "250921-14:51:58,467 nipype.workflow INFO:\n",
      "\t [MultiProc] Running 0 tasks, and 1 jobs ready. Free memory (GB): 54.38/54.38, Free processors: 24/24, Free GPU slot:1/1.\n",
      "250921-14:51:58,610 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"first_level.apply_smooth\" in \"/home/rongfei/WorkSpace/consortium/working/sub-11044/scap/first_level/apply_smooth\".\n",
      "250921-14:51:58,614 nipype.workflow INFO:\n",
      "\t [Node] Executing \"apply_smooth\" <nipype.interfaces.afni.preprocess.BlurInMask>\n",
      "250921-14:51:59,532 nipype.workflow INFO:\n",
      "\t [Node] Finished \"apply_smooth\", elapsed time 0.916388s.\n",
      "250921-14:52:00,466 nipype.workflow INFO:\n",
      "\t [Job 1] Completed (first_level.apply_smooth).\n",
      "250921-14:52:00,467 nipype.workflow INFO:\n",
      "\t [MultiProc] Running 0 tasks, and 1 jobs ready. Free memory (GB): 54.38/54.38, Free processors: 24/24, Free GPU slot:1/1.\n",
      "250921-14:52:00,609 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"first_level.model_specification\" in \"/home/rongfei/WorkSpace/consortium/working/sub-11044/scap/first_level/model_specification\".\n",
      "250921-14:52:00,614 nipype.workflow INFO:\n",
      "\t [Node] Executing \"model_specification\" <nipype.algorithms.modelgen.SpecifyModel>\n",
      "250921-14:52:00,618 nipype.workflow INFO:\n",
      "\t [Node] Finished \"model_specification\", elapsed time 0.002234s.\n",
      "250921-14:52:02,466 nipype.workflow INFO:\n",
      "\t [Job 2] Completed (first_level.model_specification).\n",
      "250921-14:52:02,467 nipype.workflow INFO:\n",
      "\t [MultiProc] Running 0 tasks, and 1 jobs ready. Free memory (GB): 54.38/54.38, Free processors: 24/24, Free GPU slot:1/1.\n",
      "250921-14:52:02,609 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"first_level.l1design\" in \"/home/rongfei/WorkSpace/consortium/working/sub-11044/scap/first_level/l1design\".\n",
      "250921-14:52:02,640 nipype.workflow INFO:\n",
      "\t [Node] Executing \"l1design\" <nipype.interfaces.fsl.model.Level1Design>\n",
      "250921-14:52:02,682 nipype.workflow INFO:\n",
      "\t [Node] Finished \"l1design\", elapsed time 0.04076s.\n",
      "250921-14:52:04,466 nipype.workflow INFO:\n",
      "\t [Job 3] Completed (first_level.l1design).\n",
      "250921-14:52:04,468 nipype.workflow INFO:\n",
      "\t [MultiProc] Running 0 tasks, and 2 jobs ready. Free memory (GB): 54.38/54.38, Free processors: 24/24, Free GPU slot:1/1.\n",
      "250921-14:52:04,604 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"first_level.l1feat\" in \"/home/rongfei/WorkSpace/consortium/working/sub-11044/scap/first_level/l1feat\".\n",
      "250921-14:52:04,604 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"first_level.l1estimate\" in \"/home/rongfei/WorkSpace/consortium/working/sub-11044/scap/first_level/l1estimate\".\n",
      "250921-14:52:04,609 nipype.workflow INFO:\n",
      "\t [Node] Executing \"l1estimate\" <nipype.interfaces.fsl.model.FEAT>\n",
      "250921-14:52:04,611 nipype.workflow INFO:\n",
      "\t [Node] Executing \"l1feat\" <nipype.interfaces.fsl.model.FEATModel>\n",
      "250921-14:52:06,467 nipype.workflow INFO:\n",
      "\t [MultiProc] Running 2 tasks, and 0 jobs ready. Free memory (GB): 53.98/54.38, Free processors: 22/24, Free GPU slot:1/1.\n",
      "                     Currently running:\n",
      "                       * first_level.l1estimate\n",
      "                       * first_level.l1feat\n",
      "250921-14:52:06,627 nipype.workflow INFO:\n",
      "\t [Node] Finished \"l1feat\", elapsed time 2.015189s.\n",
      "250921-14:52:08,467 nipype.workflow INFO:\n",
      "\t [Job 4] Completed (first_level.l1feat).\n",
      "250921-14:52:08,468 nipype.workflow INFO:\n",
      "\t [MultiProc] Running 1 tasks, and 0 jobs ready. Free memory (GB): 54.18/54.38, Free processors: 23/24, Free GPU slot:1/1.\n",
      "                     Currently running:\n",
      "                       * first_level.l1estimate\n",
      "250921-14:54:49,670 nipype.workflow INFO:\n",
      "\t [Node] Finished \"l1estimate\", elapsed time 165.060065s.\n",
      "250921-14:54:50,486 nipype.workflow INFO:\n",
      "\t [Job 5] Completed (first_level.l1estimate).\n",
      "250921-14:54:50,487 nipype.workflow INFO:\n",
      "\t [MultiProc] Running 0 tasks, and 0 jobs ready. Free memory (GB): 54.38/54.38, Free processors: 24/24, Free GPU slot:1/1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [05:56<05:56, 178.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rongfei/WorkSpace/consortium/derivatives/sub-70001/func/sub-70001_task-scap_bold_space-MNI152NLin2009cAsym_preproc.nii.gz\n",
      "/home/rongfei/WorkSpace/consortium/derivatives/sub-70001/func/sub-70001_task-scap_bold_space-MNI152NLin2009cAsym_brainmask.nii.gz\n",
      "/home/rongfei/WorkSpace/consortium/derivatives/sub-70001/func/sub-70001_task-scap_bold_space-MNI152NLin2009cAsym_preproc.nii.gz\n",
      "250921-14:54:52,621 nipype.workflow INFO:\n",
      "\t Workflow first_level settings: ['check', 'execution', 'logging', 'monitoring']\n",
      "250921-14:54:52,624 nipype.workflow INFO:\n",
      "\t Running in parallel.\n",
      "250921-14:54:52,624 nipype.workflow INFO:\n",
      "\t [MultiProc] Running 0 tasks, and 1 jobs ready. Free memory (GB): 54.38/54.38, Free processors: 24/24, Free GPU slot:1/1.\n",
      "250921-14:54:52,903 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"first_level.apply_mask\" in \"/home/rongfei/WorkSpace/consortium/working/sub-70001/scap/first_level/apply_mask\".\n",
      "250921-14:54:52,906 nipype.workflow INFO:\n",
      "\t [Node] Executing \"apply_mask\" <nipype.interfaces.fsl.maths.ApplyMask>\n",
      "250921-14:54:54,625 nipype.workflow INFO:\n",
      "\t [MultiProc] Running 1 tasks, and 0 jobs ready. Free memory (GB): 54.18/54.38, Free processors: 23/24, Free GPU slot:1/1.\n",
      "                     Currently running:\n",
      "                       * first_level.apply_mask\n",
      "250921-14:54:56,278 nipype.workflow INFO:\n",
      "\t [Node] Finished \"apply_mask\", elapsed time 3.370487s.\n",
      "250921-14:54:56,625 nipype.workflow INFO:\n",
      "\t [Job 0] Completed (first_level.apply_mask).\n",
      "250921-14:54:56,626 nipype.workflow INFO:\n",
      "\t [MultiProc] Running 0 tasks, and 1 jobs ready. Free memory (GB): 54.38/54.38, Free processors: 24/24, Free GPU slot:1/1.\n",
      "250921-14:54:56,771 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"first_level.apply_smooth\" in \"/home/rongfei/WorkSpace/consortium/working/sub-70001/scap/first_level/apply_smooth\".\n",
      "250921-14:54:56,774 nipype.workflow INFO:\n",
      "\t [Node] Executing \"apply_smooth\" <nipype.interfaces.afni.preprocess.BlurInMask>\n",
      "250921-14:54:57,646 nipype.workflow INFO:\n",
      "\t [Node] Finished \"apply_smooth\", elapsed time 0.870903s.\n",
      "250921-14:54:58,625 nipype.workflow INFO:\n",
      "\t [Job 1] Completed (first_level.apply_smooth).\n",
      "250921-14:54:58,626 nipype.workflow INFO:\n",
      "\t [MultiProc] Running 0 tasks, and 1 jobs ready. Free memory (GB): 54.38/54.38, Free processors: 24/24, Free GPU slot:1/1.\n",
      "250921-14:54:58,773 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"first_level.model_specification\" in \"/home/rongfei/WorkSpace/consortium/working/sub-70001/scap/first_level/model_specification\".\n",
      "250921-14:54:58,778 nipype.workflow INFO:\n",
      "\t [Node] Executing \"model_specification\" <nipype.algorithms.modelgen.SpecifyModel>\n",
      "250921-14:54:58,781 nipype.workflow INFO:\n",
      "\t [Node] Finished \"model_specification\", elapsed time 0.002018s.\n",
      "250921-14:55:00,625 nipype.workflow INFO:\n",
      "\t [Job 2] Completed (first_level.model_specification).\n",
      "250921-14:55:00,626 nipype.workflow INFO:\n",
      "\t [MultiProc] Running 0 tasks, and 1 jobs ready. Free memory (GB): 54.38/54.38, Free processors: 24/24, Free GPU slot:1/1.\n",
      "250921-14:55:00,770 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"first_level.l1design\" in \"/home/rongfei/WorkSpace/consortium/working/sub-70001/scap/first_level/l1design\".\n",
      "250921-14:55:00,801 nipype.workflow INFO:\n",
      "\t [Node] Executing \"l1design\" <nipype.interfaces.fsl.model.Level1Design>\n",
      "250921-14:55:00,844 nipype.workflow INFO:\n",
      "\t [Node] Finished \"l1design\", elapsed time 0.041063s.\n",
      "250921-14:55:02,625 nipype.workflow INFO:\n",
      "\t [Job 3] Completed (first_level.l1design).\n",
      "250921-14:55:02,626 nipype.workflow INFO:\n",
      "\t [MultiProc] Running 0 tasks, and 2 jobs ready. Free memory (GB): 54.38/54.38, Free processors: 24/24, Free GPU slot:1/1.\n",
      "250921-14:55:02,771 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"first_level.l1estimate\" in \"/home/rongfei/WorkSpace/consortium/working/sub-70001/scap/first_level/l1estimate\".\n",
      "250921-14:55:02,771 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"first_level.l1feat\" in \"/home/rongfei/WorkSpace/consortium/working/sub-70001/scap/first_level/l1feat\".\n",
      "250921-14:55:02,776 nipype.workflow INFO:\n",
      "\t [Node] Executing \"l1estimate\" <nipype.interfaces.fsl.model.FEAT>\n",
      "250921-14:55:02,778 nipype.workflow INFO:\n",
      "\t [Node] Executing \"l1feat\" <nipype.interfaces.fsl.model.FEATModel>\n",
      "250921-14:55:04,626 nipype.workflow INFO:\n",
      "\t [MultiProc] Running 2 tasks, and 0 jobs ready. Free memory (GB): 53.98/54.38, Free processors: 22/24, Free GPU slot:1/1.\n",
      "                     Currently running:\n",
      "                       * first_level.l1estimate\n",
      "                       * first_level.l1feat\n",
      "250921-14:55:04,743 nipype.workflow INFO:\n",
      "\t [Node] Finished \"l1feat\", elapsed time 1.9633289999999999s.\n",
      "250921-14:55:06,626 nipype.workflow INFO:\n",
      "\t [Job 4] Completed (first_level.l1feat).\n",
      "250921-14:55:06,627 nipype.workflow INFO:\n",
      "\t [MultiProc] Running 1 tasks, and 0 jobs ready. Free memory (GB): 54.18/54.38, Free processors: 23/24, Free GPU slot:1/1.\n",
      "                     Currently running:\n",
      "                       * first_level.l1estimate\n",
      "250921-14:57:45,398 nipype.workflow INFO:\n",
      "\t [Node] Finished \"l1estimate\", elapsed time 162.621056s.\n",
      "250921-14:57:46,641 nipype.workflow INFO:\n",
      "\t [Job 5] Completed (first_level.l1estimate).\n",
      "250921-14:57:46,642 nipype.workflow INFO:\n",
      "\t [MultiProc] Running 0 tasks, and 0 jobs ready. Free memory (GB): 54.38/54.38, Free processors: 24/24, Free GPU slot:1/1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [08:52<02:57, 177.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rongfei/WorkSpace/consortium/derivatives/sub-70004/func/sub-70004_task-scap_bold_space-MNI152NLin2009cAsym_preproc.nii.gz\n",
      "/home/rongfei/WorkSpace/consortium/derivatives/sub-70004/func/sub-70004_task-scap_bold_space-MNI152NLin2009cAsym_brainmask.nii.gz\n",
      "/home/rongfei/WorkSpace/consortium/derivatives/sub-70004/func/sub-70004_task-scap_bold_space-MNI152NLin2009cAsym_preproc.nii.gz\n",
      "250921-14:57:48,783 nipype.workflow INFO:\n",
      "\t Workflow first_level settings: ['check', 'execution', 'logging', 'monitoring']\n",
      "250921-14:57:48,786 nipype.workflow INFO:\n",
      "\t Running in parallel.\n",
      "250921-14:57:48,787 nipype.workflow INFO:\n",
      "\t [MultiProc] Running 0 tasks, and 1 jobs ready. Free memory (GB): 54.38/54.38, Free processors: 24/24, Free GPU slot:1/1.\n",
      "250921-14:57:49,70 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"first_level.apply_mask\" in \"/home/rongfei/WorkSpace/consortium/working/sub-70004/scap/first_level/apply_mask\".\n",
      "250921-14:57:49,73 nipype.workflow INFO:\n",
      "\t [Node] Executing \"apply_mask\" <nipype.interfaces.fsl.maths.ApplyMask>\n",
      "250921-14:57:50,789 nipype.workflow INFO:\n",
      "\t [MultiProc] Running 1 tasks, and 0 jobs ready. Free memory (GB): 54.18/54.38, Free processors: 23/24, Free GPU slot:1/1.\n",
      "                     Currently running:\n",
      "                       * first_level.apply_mask\n",
      "250921-14:57:52,504 nipype.workflow INFO:\n",
      "\t [Node] Finished \"apply_mask\", elapsed time 3.429504s.\n",
      "250921-14:57:52,788 nipype.workflow INFO:\n",
      "\t [Job 0] Completed (first_level.apply_mask).\n",
      "250921-14:57:52,789 nipype.workflow INFO:\n",
      "\t [MultiProc] Running 0 tasks, and 1 jobs ready. Free memory (GB): 54.38/54.38, Free processors: 24/24, Free GPU slot:1/1.\n",
      "250921-14:57:52,932 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"first_level.apply_smooth\" in \"/home/rongfei/WorkSpace/consortium/working/sub-70004/scap/first_level/apply_smooth\".\n",
      "250921-14:57:52,935 nipype.workflow INFO:\n",
      "\t [Node] Executing \"apply_smooth\" <nipype.interfaces.afni.preprocess.BlurInMask>\n",
      "250921-14:57:53,830 nipype.workflow INFO:\n",
      "\t [Node] Finished \"apply_smooth\", elapsed time 0.893277s.\n",
      "250921-14:57:54,788 nipype.workflow INFO:\n",
      "\t [Job 1] Completed (first_level.apply_smooth).\n",
      "250921-14:57:54,790 nipype.workflow INFO:\n",
      "\t [MultiProc] Running 0 tasks, and 1 jobs ready. Free memory (GB): 54.38/54.38, Free processors: 24/24, Free GPU slot:1/1.\n",
      "250921-14:57:54,932 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"first_level.model_specification\" in \"/home/rongfei/WorkSpace/consortium/working/sub-70004/scap/first_level/model_specification\".\n",
      "250921-14:57:54,936 nipype.workflow INFO:\n",
      "\t [Node] Executing \"model_specification\" <nipype.algorithms.modelgen.SpecifyModel>\n",
      "250921-14:57:54,939 nipype.workflow INFO:\n",
      "\t [Node] Finished \"model_specification\", elapsed time 0.002087s.\n",
      "250921-14:57:56,788 nipype.workflow INFO:\n",
      "\t [Job 2] Completed (first_level.model_specification).\n",
      "250921-14:57:56,790 nipype.workflow INFO:\n",
      "\t [MultiProc] Running 0 tasks, and 1 jobs ready. Free memory (GB): 54.38/54.38, Free processors: 24/24, Free GPU slot:1/1.\n",
      "250921-14:57:56,935 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"first_level.l1design\" in \"/home/rongfei/WorkSpace/consortium/working/sub-70004/scap/first_level/l1design\".\n",
      "250921-14:57:56,965 nipype.workflow INFO:\n",
      "\t [Node] Executing \"l1design\" <nipype.interfaces.fsl.model.Level1Design>\n",
      "250921-14:57:57,8 nipype.workflow INFO:\n",
      "\t [Node] Finished \"l1design\", elapsed time 0.041222s.\n",
      "250921-14:57:58,789 nipype.workflow INFO:\n",
      "\t [Job 3] Completed (first_level.l1design).\n",
      "250921-14:57:58,790 nipype.workflow INFO:\n",
      "\t [MultiProc] Running 0 tasks, and 2 jobs ready. Free memory (GB): 54.38/54.38, Free processors: 24/24, Free GPU slot:1/1.\n",
      "250921-14:57:58,935 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"first_level.l1feat\" in \"/home/rongfei/WorkSpace/consortium/working/sub-70004/scap/first_level/l1feat\".\n",
      "250921-14:57:58,935 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"first_level.l1estimate\" in \"/home/rongfei/WorkSpace/consortium/working/sub-70004/scap/first_level/l1estimate\".\n",
      "250921-14:57:58,939 nipype.workflow INFO:\n",
      "\t [Node] Executing \"l1estimate\" <nipype.interfaces.fsl.model.FEAT>\n",
      "250921-14:57:58,942 nipype.workflow INFO:\n",
      "\t [Node] Executing \"l1feat\" <nipype.interfaces.fsl.model.FEATModel>\n",
      "250921-14:58:00,788 nipype.workflow INFO:\n",
      "\t [MultiProc] Running 2 tasks, and 0 jobs ready. Free memory (GB): 53.98/54.38, Free processors: 22/24, Free GPU slot:1/1.\n",
      "                     Currently running:\n",
      "                       * first_level.l1estimate\n",
      "                       * first_level.l1feat\n",
      "250921-14:58:00,919 nipype.workflow INFO:\n",
      "\t [Node] Finished \"l1feat\", elapsed time 1.9760550000000001s.\n",
      "250921-14:58:02,788 nipype.workflow INFO:\n",
      "\t [Job 4] Completed (first_level.l1feat).\n",
      "250921-14:58:02,789 nipype.workflow INFO:\n",
      "\t [MultiProc] Running 1 tasks, and 0 jobs ready. Free memory (GB): 54.18/54.38, Free processors: 23/24, Free GPU slot:1/1.\n",
      "                     Currently running:\n",
      "                       * first_level.l1estimate\n",
      "250921-15:00:43,666 nipype.workflow INFO:\n",
      "\t [Node] Finished \"l1estimate\", elapsed time 164.725217s.\n",
      "250921-15:00:44,806 nipype.workflow INFO:\n",
      "\t [Job 5] Completed (first_level.l1estimate).\n",
      "250921-15:00:44,807 nipype.workflow INFO:\n",
      "\t [MultiProc] Running 0 tasks, and 0 jobs ready. Free memory (GB): 54.38/54.38, Free processors: 24/24, Free GPU slot:1/1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [11:50<00:00, 177.65s/it]\n"
     ]
    }
   ],
   "source": [
    "# Notice some ev are missings for some subjects because of the randomization during the experiment. We will remove the subjects in the group level analysis.\n",
    "# subjects = [SubjectInfo(id) for id in ids]\n",
    "\n",
    "overwrite = True\n",
    "\n",
    "for subject_info in tqdm.tqdm(selected_subjects):\n",
    "    feat_report = subject_info.get_output_dir() / 'scap.feat' / 'report.html'\n",
    "    if not feat_report.exists():\n",
    "        workflow = subject_level_modeling(subject_info)\n",
    "        run_workflow(workflow, subject_info, 'scap')\n",
    "    elif overwrite:\n",
    "        shutil.rmtree(subject_info.get_working_task_dir(\"scap\") )\n",
    "        shutil.rmtree(subject_info.get_output_dir() / 'scap.feat')\n",
    "        workflow = subject_level_modeling(subject_info)\n",
    "        run_workflow(workflow, subject_info, 'scap')\n",
    "    with open(\"completed_subjects.txt\", 'a') as f:\n",
    "        f.write(f\"{subject_info.subject_id}\\n\")\n",
    "\n",
    "# subjects = [SubjectInfo(\"sub-11106\")]\n",
    "# workflow = subject_level_modeling(subjects[0])\n",
    "# run_workflow(workflow, subjects[0], 'scap', remove_previous=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640787c3",
   "metadata": {},
   "source": [
    "# Analysis Check\n",
    "\n",
    "The whole analysis would take \n",
    "\n",
    "We now check if the statistics image generated from the pipeline matches the original file. Since everything in the pipeline is determinstic, there shouldn't be any major difference (except difference caused by float point precision, if any)\n",
    "\n",
    "Judge by the output, we've obtained the same results as the original paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12135bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from nilearn.plotting import plot_stat_map\n",
    "\n",
    "problematic_subjects = [\"sub-10998\", \"sub-10159\", \"sub-70055\", \"sub-70022\", \"sub-70048\", \"sub-10680\"]\n",
    "\n",
    "control_subjects_ids = [id for id in control_subjects_ids if id not in problematic_subjects]\n",
    "adhd_subjects_ids = [id for id in adhd_subjects_ids if id not in problematic_subjects]\n",
    "\n",
    "# Pick 2 Control and 2 ADHD subjects from the completed subjects list\n",
    "\n",
    "control_subjects = [SubjectInfo(id) for id in control_subjects_ids[:2]]\n",
    "adhd_subjects = [SubjectInfo(id) for id in adhd_subjects_ids[:2]]\n",
    "\n",
    "selected_subjects = control_subjects + adhd_subjects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3267e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the parametic modulation contrast\n",
    "def get_contrast(subject: SubjectInfo, contrast_name: str):\n",
    "    anat = subject.get_anatomical_file()\n",
    "    stat = subject.get_output_dir() / 'scap.feat' /'stats'/ f'{contrast_name}.nii.gz'\n",
    "    return anat, stat\n",
    "\n",
    "def get_contrast_downloaded(subject: SubjectInfo, contrast_name: str):\n",
    "    anat = subject.get_anatomical_file()\n",
    "    stat =  Path(\"derivatives/task/\") / subject.subject_id / 'scap.feat' /'stats'/ f'{contrast_name}.nii.gz'\n",
    "    return anat, stat\n",
    "\n",
    "for subject in selected_subjects:\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(10, 8))\n",
    "    ax[0].set_title(f'Subject {subject.subject_id} (Ours)')\n",
    "    anat, cope = get_contrast(subject, 'tstat1')\n",
    "    plot_stat_map(cope, anat, threshold=3.2, display_mode='z', cut_coords=(-5, 0, 5, 10, 15), colorbar=True, axes=ax[0])\n",
    "    ax[1].set_title(f'Subject {subject.subject_id} (Original)')\n",
    "    anat, cope = get_contrast_downloaded(subject, 'tstat1')\n",
    "    plot_stat_map(cope, anat, threshold=3.2, display_mode='z', cut_coords=(-5, 0, 5, 10, 15), colorbar=True, axes=ax[1])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
